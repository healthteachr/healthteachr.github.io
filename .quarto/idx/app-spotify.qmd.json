{"title":"Spotify Data","markdown":{"headingText":"Spotify Data","headingAttr":{"id":"sec-spotify","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nThis appendix was inspired by [Michael Mullarkey's tutorial](https://mcmullarkey.github.io/mcm-blog/posts/2022-01-07-spotify-api-r/){target=\"_blank\"}, which you can follow to make beautiful dot plots out of your own Spotify data. This tutorial doesn't require you to use Spotify; just to create a developer account so you can access their data API with <pkg>spotifyr\", \"https://www.rcharlie.com/spotifyr/\")`.\n\n```{r setup-app-spotify}\nlibrary(usethis)    # to set system environment variables\nlibrary(spotifyr)   # to access Spotify\nlibrary(tidyverse)  # for data wrangling\nlibrary(DT)         # for interactive tables\n```\n\nThe package [spotifyr](https://www.rcharlie.com/spotifyr){target=\"_blank\"} has instructions for setting up a developer account with Spotify and setting up an \"app\" so you can get authorisation codes.\n\nOnce you've set up the app, you can copy the client ID and secret to your R environment file. The easiest way to do this is with `edit_r_environ()` from <pkg>usethis</pkg>. Setting scope to \"user\" makes this available to any R project on your computer, while setting it to \"project\" makes it only available to this project.\n\n```{r, eval = FALSE}\nusethis::edit_r_environ(scope = \"user\")\n```\n\nAdd the following text to your environment file (don't delete anything already there), replacing the zeros with your personal ID and secret. Save and close the file and restart R. \n\n```\nSPOTIFY_CLIENT_ID=\"0000000000000000000000000000\"\nSPOTIFY_CLIENT_SECRET=\"0000000000000000000000000000\"\n```\n\nDouble check that it worked by typing the following into the console. Don't put it in your script unless you mean to share this confidential info.  You should see your values, not \"\", if it worked.\n\n```{r, eval = FALSE}\n# run in the console, don't save in a script\nSys.getenv(\"SPOTIFY_CLIENT_ID\")\nSys.getenv(\"SPOTIFY_CLIENT_SECRET\")\n```\n\nNow you're ready to get data from Spotify. There are several types of data that you can download. \n\n```{r, include = FALSE}\n# avoids calling spotify repeatedly and allows knitting even when you don't have a connection to Spotify\n\n#saveRDS(euro_genre2, \"R/euro_genre2.Rds\")\n\ngaga <- readRDS(\"R/gaga.Rds\")\neurovision2021 <- readRDS(\"R/eurovision2021.Rds\")\neuro_genre <- readRDS(\"R/euro_genre.Rds\")\neuro_genre2 <- readRDS(\"R/euro_genre2.Rds\")\neuro_genre200 <- readRDS(\"R/euro_genre200.Rds\")\nbtw_analysis <- readRDS(\"R/btw_analysis.Rds\")\nbtw_features <- readRDS(\"R/btw_features.Rds\")\n\n```\n\n\n## By Artist\n\nChoose your favourite artist and download their discography. Set `include_groups` to one or more of \"album\", \"single\", \"appears_on\", and \"compilation\".\n\n```{r, eval=FALSE}\ngaga <- get_artist_audio_features(\n  artist = 'Lady Gaga',\n  include_groups = \"album\"\n)\n```\n\nLet's explore the data you get back. Use `glimpse()` to see what columns are available and what type of data they have. It looks like there is a row for each of this artist's tracks.\n\nLet's answer a few simple questions first. \n\n### Tracks per Album\n\nHow many tracks are on each album? Some tracks have more than one entry in the table, so first select just the `album_name` and `track_name` columns and use `distinct()` to get rid of duplicates. Then `count()` the tracks per album. We're using `DT::datatable()` to make the table interactive. Try sorting the table by number of tracks. \n\n```{r}\ngaga %>%\n  select(album_name, track_name) %>%\n  distinct() %>%\n  count(album_name) %>%\n  datatable(colnames = c(\"Albumn Name\", \"Number of Tracks\"))\n```\n\n::: {.callout-note .try}\nUse `count()` to explore the columns `key_name`, `mode_name`, and any other non-numeric columns. \n:::\n\n### Tempo\n\nWhat sort of tempo is Lady Gaga's music? First, let's look at a very basic plot to get an overview.\n\n```{r}\nggplot(gaga, aes(tempo)) +\n  geom_histogram(binwidth = 1)\n```\n\nWhat's going on with the tracks with a tempo of 0?\n\n```{r}\ngaga %>% \n  filter(tempo == 0) %>%\n  select(album_name, track_name)\n```\n\nLooks like it's all dialogue, so we should omit these. Let's also check how variable the tempo is for multiple instances of the same track. A quick way to do this is to group by album and track, then check the `r glossary(\"standard deviation\")` of the tempo. If it's 0, this means that all of the values are identical. The bigger it is, the more the values vary. If you have a lot of data with a `r glossary(\"normal distribution\")` (like a bell curve), then about 68% of the data are within one SD of the mean, and about 95% are within 2 SDs.\n\nIf we filter to tracks with SD greater than 0 (so any variation at all), we see that most tracks have a little variation. However, if we filter to tracks with an SD greater than 1, we see a few songs with slightly different tempo, and a few with wildly different tempo. \n\n```{r}\ngaga %>%\n  # omit tracks with \"Dialogue\" in the name\n  filter(!str_detect(track_name, \"Dialogue\")) %>%\n  # check for varying tempos for same track\n  group_by(album_name, track_name) %>%\n  filter(sd(tempo) > 1) %>%\n  ungroup() %>%\n  select(album_name, track_name, tempo) %>%\n  arrange(album_name, track_name)\n```\n\nYou can deal with these in any way you choose. Filter out some versions of the songs or listen to them to see which value you agree with and change the others. Here, we'll deal with it by averaging the values for each track. This will also remove the tiny differences in the majority of duplicate tracks. Now we're ready to plot.\n\n```{r}\ngaga %>%\n  filter(tempo > 0) %>%\n  group_by(album_name, track_name) %>%\n  summarise(tempo = round(mean(tempo)),\n            .groups = \"drop\") %>%\n  ungroup() %>%\n  ggplot(aes(x = tempo, fill = ..x..)) +\n  geom_histogram(binwidth = 4, show.legend = FALSE) +\n  scale_fill_gradient(low = \"#521F64\", high = \"#E8889C\") +\n  labs(x = \"Beats per minute\",\n       y = \"Number of tracks\",\n       title = \"Tempo of Lady Gaga Tracks\")\n```\n\n::: {.callout-note .try}\nCan you see how we made the gradient fill for the histograms? Since the x-value of each bar depends on the binwidth, you have to use the code `..x..` in the mapping (not `tempo`) to make the fill correspond to each bar's value. \n:::\n\nThis looks OK, but maybe we want a more striking plot. Let's make a custom plot style and assign it to `gaga_style` in case we want to use it again. Then add it to the shortcut function, `last_plot()` to avoid having to retype the code for the last plot we created.\n\n```{r}\n# define style\ngaga_style <- theme(\n  plot.background = element_rect(fill = \"black\"),\n  text = element_text(color = \"white\", size = 11),\n  panel.background = element_rect(fill = \"black\"),\n  panel.grid.major.x = element_blank(),\n  panel.grid.minor.x = element_blank(),\n  panel.grid.major.y = element_line(colour = \"white\", size = 0.2),\n  panel.grid.minor.y = element_line(colour = \"white\", size = 0.2),\n  axis.text = element_text(color = \"white\"),\n  plot.title = element_text(hjust = 0.5)\n)\n\n## add it to the last plot created\nlast_plot() + gaga_style\n\n```\n\n\n## By Playlist\n\nYou need to know the \"uri\" of a public playlist to access data on it. You can get this by copying the link to the playlist and selecting the 22 characters between \"https://open.spotify.com/playlist/\" and \"?si=...\". Let's have a look at the Eurovision 2021 playlist.\n\n```{r, eval = FALSE}\neurovision2021 <- get_playlist_audio_features(\n  playlist_uris = \"37i9dQZF1DWVCKO3xAlT1Q\"\n)\n```\n\nUse `glimpse()` and `count()` to explore the structure of this table.\n\n### Track ratings\n\nEach track has several ratings: danceability, energy, speechiness, acousticness, instrumentalness, liveness, and valence. I'm not sure how these are determined (almost certainly by an algorithm). Let's select the track names and these columns to have a look.\n\n```{r}\neurovision2021 %>%\n  select(track.name, danceability, energy, speechiness:valence) %>%\n  datatable()\n```\n\nWhat was the general mood of Eurovision songs in 2021? Let's use plots to assess. First, we need to get the data into long format to make it easier to plot multiple attributes.\n\n```{r}\nplaylist_attributes <- eurovision2021 %>%\n  select(track.name, danceability, energy, speechiness:valence) %>%\n  pivot_longer(cols = danceability:valence,\n               names_to = \"attribute\",\n               values_to = \"rating\")\n```\n\nWhen we plot everything on the same plot, instrumentalness has such a consistently low value that all the other attributes disappear, \n\n```{r}\nggplot(playlist_attributes, aes(x = rating, colour = attribute)) +\n  geom_density()\n```\n\nYou can solve this by putting each attribute into its own facet and letting the y-axis differ between plots by setting `scales = \"free_y\"`. Now it's easier to see that Eurovision songs tend to have pretty high danceability and energy.\n\n```{r}\n#| playlist-attributes-facet,\n#| fig.width = 10, fig.height = 5,\n#| fig.cap = \"Seven track attributes for the playlist 'Eurovision 2021'\"\nggplot(playlist_attributes, aes(x = rating, colour = attribute)) +\n  geom_density(show.legend = FALSE) +\n  facet_wrap(~attribute, scales = \"free_y\", nrow = 2)\n```\n\n### Popularity\n\nLet's look at how these attributes relate to track popularity. We'll exclude instrumentalness, since it doesn't have much variation.\n\n```{r}\npopularity <- eurovision2021 %>%\n  select(track.name, track.popularity,\n         acousticness, danceability, energy, \n         liveness, speechiness, valence) %>%\n  pivot_longer(cols = acousticness:valence,\n               names_to = \"attribute\",\n               values_to = \"rating\")\n```\n\n\n```{r}\n#| playlist-popularity,\n#| fig.width = 7.5, fig.height = 5,\n#| fig.cap = \"The relationship between track attributes and popularity.\"\nggplot(popularity, aes(x = rating, y = track.popularity, colour = attribute)) +\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  geom_smooth(method = lm, formula = y~x, show.legend = FALSE) +\n  facet_wrap(~attribute, scales = \"free_x\", nrow = 2) +\n  labs(x = \"Attribute Value\",\n       y = \"Track Popularity\")\n```\n\n\n### Nested data\n\nSome of the columns in this table contain more tables. For example, each entry in the `track.artist` column contains a table with columns `href`, `id`, `name`, `type`, `uri`, and `external_urls.spotify`. Use `unnest()` to extract these tables. If there is more than one artist for a track, this will expand the table. For example, the track \"Adrenalina\" has two rows now, one for Senhit and one for Flo Rida.\n\n```{r}\neurovision2021 %>%\n  unnest(track.artists) %>%\n  select(track = track.name, \n         artist = name, \n         popularity = track.popularity) %>%\n  datatable()\n\n```\n\n\n::: {.callout-note .try}\nIf you're a Eurovision nerd (like Emily), try downloading playlists from several previous years and visualise trends. See if you can find lists of the [scores for each year](https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2021){target=\"_blank\"} and join the data to see what attributes are most related to points.\n:::\n\n## By Genre\n\nSelect the first 20 artists in the genre \"eurovision\". So that people don't spam the Spotify API, you are limited to up to 50 artists per request.\n\n```{r, eval=FALSE}\neuro_genre <- get_genre_artists(\n  genre = \"eurovision\",\n  limit = 20,\n  offset = 0\n)\n```\n\n\n```{r}\neuro_genre %>%\n  select(name, popularity, followers.total) %>%\n  datatable()\n```\n\nNow you can select the next 20 artists, incrementing the offset by 20, join that to the first table, and process the data.\n\n```{r, eval=FALSE}\neuro_genre2 <- get_genre_artists(\n  genre = \"eurovision\",\n  limit = 20,\n  offset = 20\n)\n```\n\n\n```{r}\nbind_rows(euro_genre, euro_genre2) %>%\n  select(name, popularity, followers.total) %>%\n  datatable()\n```\n\n### Repeated calls\n\nThere is a programmatic way to make several calls to a function that limits you. You usually want to set this up so that you are waiting a few seconds or minutes between calls so that you don't get locked out (depending on how strict the API is). Use `map_df()` to automatically join the results into one big table.\n\n```{r, eval = FALSE}\n# create a slow version of get_genre_artists \n# delays 2 seconds after running\nslow_get_genre_artists <- slowly(get_genre_artists, \n                                 rate = rate_delay(2))\n\n# set 4 offsets from 0 to 150 by 50\noffsets <- seq(0, 150, 50)\n\n# run the slow function once for each offset\neuro_genre200 <- map_df(.x = offsets, \n                       .f = ~slow_get_genre_artists(\"eurovision\", \n                                                    limit = 50,\n                                                    offset = .x))\n```\n\n\n```{r}\neuro_genre200 %>%\n  select(name, popularity, followers.total) %>%\n  arrange(desc(followers.total)) %>%\n  datatable()\n```\n\n\n## By Track\n\nYou can get even more info about a specific track if you know its Spotify ID. You can get this from an artist, album, or playlist tables. \n\n```{r}\n# get the ID for Born This Way from the original album\nbtw_id <- gaga %>%\n  filter(track_name == \"Born This Way\", \n         album_name == \"Born This Way\") %>%\n  pull(track_id)\n```\n\n### Features\n\nFeatures are a list of summary attributes of the track. These are also included in the previous tables, so this function isn't very useful unless you are getting track IDs directly.\n\n```{r, eval = FALSE}\nbtw_features <- get_track_audio_features(btw_id)\n```\n\n```{r, echo = FALSE}\nstr(btw_features)\n```\n\n### Analysis\n\nThe analysis gives you seven different tables of details about the track. Use the `names()` function to see their names and look at each object to see what information it contains.\n\n```{r, eval = FALSE}\nbtw_analysis <- get_track_audio_analysis(btw_id)\n```\n\n```{r, echo = FALSE}\nnames(btw_analysis)\n```\n\n* `meta` gives you a list of some info about the analysis.\n* `track` gives you a list of attributes, including `duration`, `loudness`, `end_of_fade_in`, `start_of_fade_out`, and `time_signature`. Some of this info was available in the previous tables.\n* `bars`, `beats`, and `tatums` are tables with the start, duration and confidence for each bar, beat, or tatum of music (whatever a \"tatum\" is).\n* `sections` is a table with the start, duration, loudness, tempo, key, mode, time signature for each section of music, along with confidence measures of each. \n* `segments` is a table with information about loudness, pitch and timbre of segments of analysis, which tend to be around 0.2 (seconds?)\n\nYou can use this data to map a song. \n\n```{r}\n#| track-segment-map,\n#| fig.cap = \"Use data from the segments table of a track analysis to plot loudness over time.\"\nggplot(btw_analysis$segments, aes(x = start, \n                                  y = loudness_start, \n                                  color = loudness_start)) +\n  geom_point(show.legend = FALSE) +\n  scale_colour_gradient(low = \"red\", high = \"purple\") +\n  scale_x_continuous(breaks = seq(0, 300, 30)) +\n  labs(x = \"Seconds\",\n       y = \"Loudness\",\n       title = \"Loudness Map for 'Born This Way'\") +\n  gaga_style\n```\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"kable","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"wrap","code-link":true,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["include/healthteachr.css","include/webex.css","include/style.css"],"highlight-style":"a11y","include-after-body":["include/webex.js","include/script.js"],"output-file":"app-spotify.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","bibliography":["include/book.bib","include/packages.bib"],"csl":"include/apa.csl","theme":{"light":["flatly","include/light.scss"],"dark":["darkly","include/dark.scss"]},"number-depth":3,"code-copy":"hover"},"extensions":{"book":{"multiFile":true}}}}}